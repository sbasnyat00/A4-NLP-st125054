{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-8kZmr4ItGUj"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from   random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set GPU device\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "\n",
    "This dataset was taken from Kaggle (https://www.kaggle.com/datasets/nishantsingh96/refined-bookcorpus-dataset) for Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file\n",
    "csv_path = \"bookcorpus_80k.csv\"\n",
    "dataset = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = dataset['text'].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [x.strip().lower() for x in sentences if isinstance(x, str)] #lower case\n",
    "text = [re.sub(r\"[^\\w\\s]\", '', x) for x in text]  #clean all symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its probably that dammed james woman the judge thought how shed gotten into the picture was a puzzle but it probably had something to do with old busybody amy lane it seemed half the stuff that ever went on in the sister bay area had her nose poked into it _____\n",
      "['its', 'probably', 'that', 'dammed', 'james', 'woman', 'the', 'judge', 'thought', 'how', 'shed', 'gotten', 'into', 'the', 'picture', 'was', 'a', 'puzzle', 'but', 'it', 'probably', 'had', 'something', 'to', 'do', 'with', 'old', 'busybody', 'amy', 'lane', 'it', 'seemed', 'half', 'the', 'stuff', 'that', 'ever', 'went', 'on', 'in', 'the', 'sister', 'bay', 'area', 'had', 'her', 'nose', 'poked', 'into', 'it']\n"
     ]
    }
   ],
   "source": [
    "for sentence in text:\n",
    "    print(sentence, \"_____\")\n",
    "    words = sentence.split()\n",
    "    print(words)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making vocabs\n",
    "\n",
    "Before making the vocabs, let's remove all question marks and perios, etc, then turn everything to lowercase, and then simply split the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125054/.conda/envs/pytorch_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Creating word2id: 114004it [00:00, 1481063.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "114008"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Combine everything into one to make vocab\n",
    "word_list = list(set(\" \".join(text).split()))\n",
    "word2id = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}  # special tokens\n",
    "\n",
    "# Create the word2id in a single pass\n",
    "for i, w in tqdm(enumerate(word_list), desc=\"Creating word2id\"):\n",
    "    word2id[w] = i + 4  # because 0-3 are already occupied\n",
    "\n",
    "# Precompute the id2word mapping (this can be done once after word2id is fully populated)\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "vocab_size = len(word2id)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 80000/80000 [00:01<00:00, 52974.99it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2id)\n",
    "\n",
    "# List of all tokens for the whole text\n",
    "token_list = []\n",
    "\n",
    "# Process sentences more efficiently\n",
    "for sentence in tqdm(text, desc=\"Processing sentences\"):\n",
    "    token_list.append([word2id[word] for word in sentence.split()])\n",
    "\n",
    "# Now token_list contains the tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"it's probably that dammed james woman, the judge thought. how she'd gotten into the picture was a puzzle, but it probably had something to do with old busybody amy lane. it seemed half the stuff that ever went on in the sister bay area had her nose poked into it.\",\n",
       " \"the great range of the tolle'fornosse'veetri, had kept the east and the west of the continent separate from the day of creation until quite recent times, recent in comparison to the true age of the world that is. for the tolle'fornosse'veetri had slowly been eroded by severe weather and movement of the continental plates, these natural forces had contributed to the reduced height of the mountains range until eventually the high mountain passes had become free from snow and ice for a few weeks each year allowing precarious passage across them. even so, these paths were not for the faint of heart. the weather was unpredictable as were natural dangers like hidden crevasses and avalanches and many wild things sought refuge in the highest places, many things that do not welcome the presence of t'iea, or of men, or any other race for that matter.\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take a look at sentences\n",
    "sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZ42SFLKtsv_",
    "outputId": "16c28ac8-8349-48ab-f1d3-a9431e658349",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[64632,\n",
       "  59595,\n",
       "  38545,\n",
       "  20778,\n",
       "  95621,\n",
       "  21511,\n",
       "  66893,\n",
       "  24097,\n",
       "  48879,\n",
       "  113403,\n",
       "  30933,\n",
       "  60384,\n",
       "  50098,\n",
       "  66893,\n",
       "  72397,\n",
       "  73144,\n",
       "  50205,\n",
       "  8256,\n",
       "  85390,\n",
       "  11678,\n",
       "  59595,\n",
       "  17151,\n",
       "  110962,\n",
       "  95231,\n",
       "  5460,\n",
       "  18539,\n",
       "  45777,\n",
       "  19361,\n",
       "  104864,\n",
       "  6051,\n",
       "  11678,\n",
       "  67972,\n",
       "  1861,\n",
       "  66893,\n",
       "  37763,\n",
       "  38545,\n",
       "  13436,\n",
       "  77919,\n",
       "  454,\n",
       "  19120,\n",
       "  66893,\n",
       "  103747,\n",
       "  20218,\n",
       "  11958,\n",
       "  17151,\n",
       "  17210,\n",
       "  72535,\n",
       "  13217,\n",
       "  50098,\n",
       "  11678],\n",
       " [66893,\n",
       "  70475,\n",
       "  34316,\n",
       "  80975,\n",
       "  66893,\n",
       "  60422,\n",
       "  17151,\n",
       "  49290,\n",
       "  66893,\n",
       "  73193,\n",
       "  110853,\n",
       "  66893,\n",
       "  77122,\n",
       "  80975,\n",
       "  66893,\n",
       "  28245,\n",
       "  81020,\n",
       "  104129,\n",
       "  66893,\n",
       "  43796,\n",
       "  80975,\n",
       "  106654,\n",
       "  5036,\n",
       "  21517,\n",
       "  50452,\n",
       "  85431,\n",
       "  50452,\n",
       "  19120,\n",
       "  69605,\n",
       "  95231,\n",
       "  66893,\n",
       "  51052,\n",
       "  18509,\n",
       "  80975,\n",
       "  66893,\n",
       "  82533,\n",
       "  38545,\n",
       "  49176,\n",
       "  75040,\n",
       "  66893,\n",
       "  60422,\n",
       "  17151,\n",
       "  27259,\n",
       "  29103,\n",
       "  45021,\n",
       "  12273,\n",
       "  33675,\n",
       "  34229,\n",
       "  110853,\n",
       "  105479,\n",
       "  80975,\n",
       "  66893,\n",
       "  97286,\n",
       "  98709,\n",
       "  62778,\n",
       "  11039,\n",
       "  75807,\n",
       "  17151,\n",
       "  53159,\n",
       "  95231,\n",
       "  66893,\n",
       "  32121,\n",
       "  63064,\n",
       "  80975,\n",
       "  66893,\n",
       "  13839,\n",
       "  34316,\n",
       "  5036,\n",
       "  74801,\n",
       "  66893,\n",
       "  77670,\n",
       "  53621,\n",
       "  107704,\n",
       "  17151,\n",
       "  15392,\n",
       "  38986,\n",
       "  104129,\n",
       "  88024,\n",
       "  110853,\n",
       "  14136,\n",
       "  75040,\n",
       "  50205,\n",
       "  9708,\n",
       "  63686,\n",
       "  65541,\n",
       "  9994,\n",
       "  434,\n",
       "  12464,\n",
       "  47123,\n",
       "  109482,\n",
       "  87339,\n",
       "  70117,\n",
       "  5111,\n",
       "  62778,\n",
       "  94868,\n",
       "  86163,\n",
       "  50411,\n",
       "  75040,\n",
       "  66893,\n",
       "  105409,\n",
       "  80975,\n",
       "  11179,\n",
       "  66893,\n",
       "  34229,\n",
       "  73144,\n",
       "  94700,\n",
       "  110776,\n",
       "  86163,\n",
       "  11039,\n",
       "  71885,\n",
       "  82714,\n",
       "  76043,\n",
       "  106869,\n",
       "  110853,\n",
       "  83627,\n",
       "  110853,\n",
       "  48734,\n",
       "  54783,\n",
       "  21432,\n",
       "  4545,\n",
       "  8845,\n",
       "  19120,\n",
       "  66893,\n",
       "  43482,\n",
       "  88394,\n",
       "  48734,\n",
       "  21432,\n",
       "  38545,\n",
       "  5460,\n",
       "  50411,\n",
       "  54433,\n",
       "  66893,\n",
       "  105092,\n",
       "  80975,\n",
       "  97459,\n",
       "  43921,\n",
       "  80975,\n",
       "  43753,\n",
       "  43921,\n",
       "  70199,\n",
       "  5070,\n",
       "  76747,\n",
       "  75040,\n",
       "  38545,\n",
       "  67652]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take a look at token_list\n",
    "token_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its\n",
      "probably\n",
      "that\n",
      "dammed\n",
      "james\n",
      "woman\n",
      "the\n",
      "judge\n",
      "thought\n",
      "how\n",
      "shed\n",
      "gotten\n",
      "into\n",
      "the\n",
      "picture\n",
      "was\n",
      "a\n",
      "puzzle\n",
      "but\n",
      "it\n",
      "probably\n",
      "had\n",
      "something\n",
      "to\n",
      "do\n",
      "with\n",
      "old\n",
      "busybody\n",
      "amy\n",
      "lane\n",
      "it\n",
      "seemed\n",
      "half\n",
      "the\n",
      "stuff\n",
      "that\n",
      "ever\n",
      "went\n",
      "on\n",
      "in\n",
      "the\n",
      "sister\n",
      "bay\n",
      "area\n",
      "had\n",
      "her\n",
      "nose\n",
      "poked\n",
      "into\n",
      "it\n"
     ]
    }
   ],
   "source": [
    "#testing one sentence\n",
    "for tokens in token_list[0]:\n",
    "    print(id2word[tokens])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loader\n",
    "\n",
    "We gonna make dataloader.  Inside here, we need to make two types of embeddings: **token embedding** and **segment embedding**\n",
    "\n",
    "1. **Token embedding** - Given “The cat is walking. The dog is barking”, we add [CLS] and [SEP] >> “[CLS] the cat is walking [SEP] the dog is barking”. \n",
    "\n",
    "2. **Segment embedding**\n",
    "A segment embedding separates two sentences, i.e., [0 0 0 0 1 1 1 1 ]\n",
    "\n",
    "3. **Masking**\n",
    "As mentioned in the original paper, BERT randomly assigns masks to 15% of the sequence. In this 15%, 80% is replaced with masks, while 10% is replaced with random tokens, and the rest 10% is left as is.  Here we specified `max_pred` \n",
    "\n",
    "4. **Padding**\n",
    "Once we mask, we will add padding. For simplicity, here we padded until some specified `max_len`. \n",
    "\n",
    "Note:  `positive` and `negative` are just simply counts to keep track of the batch size.  `positive` refers to two sentences that are really next to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "max_mask   = 5  # max masked tokens when 15% exceed, it will only be max_pred\n",
    "max_len    = 1000 # maximum of length to be padded; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TtyOOmRntu8w"
   },
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0  #count of batch size;  we want to have half batch that are positive pairs (i.e., next sentence pairs)\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        \n",
    "        #randomly choose two sentence so we can put [SEP]\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        #retrieve the two sentences\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "        #1. token embedding - append CLS and SEP\n",
    "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
    "\n",
    "        #2. segment embedding - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        #3. mask language modeling\n",
    "        #masked 15%, but should be at least 1 but does not exceed max_mask\n",
    "        n_pred =  min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        #get the pos that excludes CLS and SEP and shuffle them\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]'] and token != word2id['[SEP]']]\n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        #simply loop and change the input_ids to [MASK]\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)  #remember the position\n",
    "            masked_tokens.append(input_ids[pos]) #remember the tokens\n",
    "            #80% replace with a [MASK], but 10% will replace with a random token\n",
    "            if random() < 0.1:  # 10%\n",
    "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "                input_ids[pos] = word2id[id2word[index]] # replace\n",
    "            elif random() < 0.9:  # 80%\n",
    "                input_ids[pos] = word2id['[MASK]'] # make mask\n",
    "            else:  #10% do nothing\n",
    "                pass\n",
    "\n",
    "        # pad the input_ids and segment ids until the max len\n",
    "        n_pad = max_len - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # pad the masked_tokens and masked_pos to make sure the lenth is max_mask\n",
    "        if max_mask > n_pred:\n",
    "            n_pad = max_mask - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        #check if first sentence is really comes before the second sentence\n",
    "        #also make sure positive is exactly half the batch size\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "            \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Q7_HC-Y0jC3K"
   },
   "outputs": [],
   "source": [
    "batch = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len of batch\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can deconstruct using map and zip\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "Recall that BERT only uses the encoder.\n",
    "\n",
    "BERT has the following components:\n",
    "\n",
    "- Embedding layers\n",
    "- Attention Mask\n",
    "- Encoder layer\n",
    "- Multi-head attention\n",
    "- Scaled dot product attention\n",
    "- Position-wise feed-forward network\n",
    "- BERT (assembling all the components)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Embedding\n",
    "\n",
    "Here we simply generate the positional embedding, and sum the token embedding, positional embedding, and segment embedding together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, n_segments, d_model, device):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        #x, seg: (bs, len)\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long).to(self.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "s1PGksqBNuZM"
   },
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k, device):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1).to(device)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 4\n",
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "GPU 1: NVIDIA GeForce RTX 2080 Ti\n",
      "GPU 2: NVIDIA GeForce RTX 2080 Ti\n",
      "GPU 3: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check available GPUs\n",
    "print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_attn_pad_mask(input_ids, input_ids, device).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Encoder\n",
    "\n",
    "The encoder has two main components: \n",
    "\n",
    "- Multi-head Attention\n",
    "- Position-wise feed-forward network\n",
    "\n",
    "First let's make the wrapper called `EncoderLayer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, d_k, device):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(n_heads, d_model, d_k, device)\n",
    "        self.pos_ffn       = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the scaled dot attention, to be used inside the multihead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k, device):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_k])).to(device)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / self.scale # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the parameters first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 6    # number of Encoder of Encoder Layer\n",
    "n_heads  = 8    # number of heads in Multi-Head Attention\n",
    "d_model  = 768  # Embedding Size\n",
    "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Multiheadattention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_k, device):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_k\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, self.d_v * n_heads)\n",
    "        self.device = device\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention(self.d_k, self.device)(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(self.n_heads * self.d_v, self.d_model, device=self.device)(context)\n",
    "        return nn.LayerNorm(self.d_model, device=self.device)(output + residual), attn # output: [batch_size x len_q x d_model]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the PoswiseFeedForwardNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(F.gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Putting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "OZ0TJ84W4SZw"
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, n_layers, n_heads, d_model, d_ff, d_k, n_segments, vocab_size, max_len, device):\n",
    "        super(BERT, self).__init__()\n",
    "        self.params = {'n_layers': n_layers, 'n_heads': n_heads, 'd_model': d_model,\n",
    "                       'd_ff': d_ff, 'd_k': d_k, 'n_segments': n_segments,\n",
    "                       'vocab_size': vocab_size, 'max_len': max_len}\n",
    "        self.embedding = Embedding(vocab_size, max_len, n_segments, d_model, device)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(n_heads, d_model, d_ff, d_k, device) for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        \n",
    "        # 1. predict next sentence\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        # 2. predict the masked token\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_nsp\n",
    "    \n",
    "    def get_last_hidden_state(self, input_ids, segment_ids):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "n_layers = 12    # number of Encoder of Encoder Layer\n",
    "n_heads  = 12    # number of heads in Multi-Head Attention\n",
    "d_model  = 768  # Embedding Size\n",
    "d_ff = d_model * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2\n",
    "\n",
    "num_epoch = 1000\n",
    "model = BERT(\n",
    "    n_layers, \n",
    "    n_heads, \n",
    "    d_model, \n",
    "    d_ff, \n",
    "    d_k, \n",
    "    n_segments, \n",
    "    vocab_size, \n",
    "    max_len, \n",
    "    device\n",
    ").to(device)  # Move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UAG3SEP4UbU",
    "outputId": "bc6f202f-df37-4fac-843c-fb86bdb777b2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 loss = 117.719650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  10%|█         | 101/1000 [00:35<05:15,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 loss = 7.738173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  20%|██        | 201/1000 [01:10<04:42,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200 loss = 3.978899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  30%|███       | 301/1000 [01:45<04:07,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300 loss = 3.881724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  40%|████      | 401/1000 [02:20<03:32,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 400 loss = 3.814898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  50%|█████     | 501/1000 [02:55<02:57,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500 loss = 3.724886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  60%|██████    | 601/1000 [03:31<02:22,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600 loss = 3.705894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  70%|███████   | 701/1000 [04:06<01:47,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 700 loss = 3.698636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  80%|████████  | 801/1000 [04:41<01:11,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 800 loss = 3.705327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  90%|█████████ | 901/1000 [05:17<00:35,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 900 loss = 3.702112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 1000/1000 [05:52<00:00,  2.84it/s]\n"
     ]
    }
   ],
   "source": [
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "# Move inputs to GPU\n",
    "input_ids = input_ids.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "masked_tokens = masked_tokens.to(device)\n",
    "masked_pos = masked_pos.to(device)\n",
    "isNext = isNext.to(device)\n",
    "\n",
    "# Wrap the epoch loop with tqdm\n",
    "for epoch in tqdm(range(num_epoch), desc=\"Training Epochs\"):\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)    \n",
    "    #logits_lm: (bs, max_mask, vocab_size) ==> (6, 5, 34)\n",
    "    #logits_nsp: (bs, yes/no) ==> (6, 2)\n",
    "\n",
    "    #1. mlm loss\n",
    "    #logits_lm.transpose: (bs, vocab_size, max_mask) vs. masked_tokens: (bs, max_mask)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    #2. nsp loss\n",
    "    #logits_nsp: (bs, 2) vs. isNext: (bs, )\n",
    "    loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n",
    "    \n",
    "    #3. combine loss\n",
    "    loss = loss_lm + loss_nsp\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch:', '%02d' % (epoch), 'loss =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to bert_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model after training\n",
    "torch.save(model.state_dict(), 'bert_model.pth')\n",
    "print(\"Model saved to bert_model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference\n",
    "\n",
    "Since our dataset is very small, it won't work very well, but just for the sake of demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'remember', 'now', 'that', 'he', 'told', 'me', 'a', 'while', 'ago', 'that', 'the', 'futons', '[MASK]', 'been', 'regularly', 'bugging', 'him', 'about', 'the', 'body', 'of', 'that', 'old', 'woman', 'found', 'in', 'the', 'construction', 'site', 'he', 'was', 'working', 'on', 'at', 'the', 'beginning', 'of', 'the', 'year', 'asking', 'him', 'the', 'same', 'questions', 'over', 'and', 'over', 'he', 'said', 'he', 'gives', 'the', 'same', 'answers', 'over', 'and', 'over', 'but', 'confessed', 'that', 'it', 'felt', 'like', 'they', 'suspected', 'he', 'either', 'had', 'something', 'to', 'do', 'with', 'the', 'ladys', 'death', 'or', 'knew', '[MASK]', 'who', 'had', '[SEP]', 'andre', 'hung', 'up', 'the', 'phone', 'but', 'he', 'didnt', 'turn', 'back', 'to', 'her', 'instead', 'he', 'sat', 'up', 'staring', 'at', 'the', 'cell', 'phone', 'in', 'his', 'hands', 'as', 'if', 'he', 'couldnt', 'quite', 'figure', 'what', 'to', 'do', 'with', 'it', 'stacey', 'sat', 'up', 'too', 'moving', 'to', 'put', 'her', 'arms', '[MASK]', 'his', 'shoulders', 'andre', 'whats', 'wrong', '[SEP]']\n",
      "Masked Tokens (Ground Truth):  ['have', 'cops', 'in', 'around', 'someone']\n",
      "Masked Tokens List (Ground Truth):  [38031, 86448, 32999, 30358, 108046]\n",
      "Predicted Masked Tokens (Words):  ['enforcing', 'enforcing', 'enforcing', 'enforcing', 'enforcing']\n",
      "Predicted Masked Tokens List:  [18294, 18294, 18294, 18294, 18294]\n",
      "Ground Truth isNext: True\n",
      "Predicted isNext: False\n"
     ]
    }
   ],
   "source": [
    "# Predict Masked Tokens and NSP\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[2]))\n",
    "\n",
    "print([id2word[w.item()] for w in input_ids[0] if id2word[w.item()] != '[PAD]'])\n",
    "\n",
    "# Move inputs to device\n",
    "input_ids = input_ids.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "masked_tokens = masked_tokens.to(device)\n",
    "masked_pos = masked_pos.to(device)\n",
    "isNext = isNext.to(device)\n",
    "\n",
    "# Get model predictions\n",
    "logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
    "#logits_lm:  (1, max_mask, vocab_size) ==> (1, 5, 34)\n",
    "#logits_nsp: (1, yes/no) ==> (1, 2)\n",
    "\n",
    "#predict masked tokens\n",
    "#max the probability along the vocab dim (2), [1] is the indices of the max, and [0] is the first value'\n",
    "# Predict masked tokens\n",
    "logits_lm = logits_lm.data.cpu().max(2)[1][0].numpy()  # Get highest probability words\n",
    "\n",
    "print('Masked Tokens (Ground Truth): ', [id2word[int(pos.item())] for pos in masked_tokens[0]])\n",
    "print('Masked Tokens List (Ground Truth): ', [int(pos.item()) for pos in masked_tokens[0]])\n",
    "print('Predicted Masked Tokens (Words): ', [id2word[int(pos)] for pos in logits_lm.tolist()])\n",
    "print('Predicted Masked Tokens List: ', logits_lm.tolist())\n",
    "\n",
    "# Predict NSP\n",
    "logits_nsp = logits_nsp.cpu().data.max(1)[1].item()  # Convert to scalar\n",
    "\n",
    "print('Ground Truth isNext:', bool(isNext.item()))\n",
    "print('Predicted isNext:', bool(logits_nsp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying a bigger dataset should be able to see the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Sentence Embedding with Sentence BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing SNLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': ['A historian and his friend digging in the mines to look for more fossils for study.', 'Boy getting helped onto a merry-go-round.', 'A man making a contemplative pose in a laundry room.', 'A foreign man is preparing an ethnic dish.', 'Two young blond girls are eating with chopsticks.'], 'hypothesis': ['the historian is digging with his friend for study.', 'A boy is riding a donkey.', 'A man is outside on the patio.', 'A foreign man is preparing an ethnic dish, filled with colorful vegetables and fruit.', 'Some girls are eating Chinese food.'], 'label': [1, 2, 2, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "#loading SNLI datasets\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "LargeDataset = load_dataset(\"snli\")\n",
    "\n",
    "#randomly sampling 50K rows from the train split\n",
    "dataset = LargeDataset[\"train\"].shuffle(seed=42).select(range(50000))\n",
    "\n",
    "print(dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'hypothesis', 'label'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercase\n",
    "premise = [x.strip().lower() for x in dataset[\"premise\"] if isinstance(x, str)]\n",
    "hypothesis = [x.strip().lower() for x in dataset[\"hypothesis\"] if isinstance(x, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = [re.sub(r\"[^\\w\\s]\", '', x) for x in premise]  # Remove symbols\n",
    "hypothesis = [re.sub(r\"[^\\w\\s]\", '', x) for x in hypothesis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building Vocab\n",
    "\n",
    "word_list = list(set(\" \".join(premise + hypothesis).split()))\n",
    "\n",
    "#initializing word2id with special tokens\n",
    "word2id_SBERT = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "\n",
    "for i, w in enumerate(word_list):\n",
    "    word2id_SBERT[w] = i + 4  #starting indexing after special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Premises: 100%|██████████| 50000/50000 [00:00<00:00, 178581.94it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2id_SBERT)\n",
    "\n",
    "tokenized_premise = []\n",
    "tokenized_hypothesis = []\n",
    "\n",
    "for sentence in tqdm(premise, desc=\"Tokenizing Premises\"):\n",
    "    tokenized_premise.append([word2id_SBERT.get(word, word2id_SBERT[\"[PAD]\"]) for word in sentence.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Hypotheses: 100%|██████████| 50000/50000 [00:00<00:00, 115287.42it/s]\n"
     ]
    }
   ],
   "source": [
    "for sentence in tqdm(hypothesis, desc=\"Tokenizing Hypotheses\"):\n",
    "    tokenized_hypothesis.append([word2id_SBERT.get(word, word2id_SBERT[\"[PAD]\"]) for word in sentence.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_premise)):\n",
    "    tokenized_premise[i] = tokenized_premise[i][:128] + [word2id_SBERT[\"[PAD]\"]] * (128 - min(len(tokenized_premise[i]), 128))\n",
    "    tokenized_hypothesis[i] = tokenized_hypothesis[i][:128] + [word2id_SBERT[\"[PAD]\"]] * (128 - min(len(tokenized_hypothesis[i]), 128))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized = dataset.add_column(\"input_ids1\", tokenized_premise)   \n",
    "dataset_tokenized = dataset_tokenized.add_column(\"input_ids2\", tokenized_hypothesis)  #adding tockenized\n",
    "dataset_tokenized = dataset_tokenized.remove_columns([\"premise\", \"hypothesis\"])   #removing untockenozed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids1', 'input_ids2'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_dataset = SNLIDataset(dataset_tokenized)\n",
    "\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading saved model from Task1\n",
    "model = BERT(n_layers, n_heads, d_model, d_ff, d_k, n_segments, 114008, max_len, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_model_path = \"bert_model.pth\"\n",
    "model.load_state_dict(torch.load(bert_model_path, map_location=device))\n",
    "model.eval() #Ensuring BERT embeddings stay the same while training SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_bias: torch.Size([114008]), requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}, requires_grad={param.requires_grad}\")\n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#siamese network for Sentence-BERT (SBERT)\n",
    "\n",
    "#Implementing the SBERT class\n",
    "\n",
    "class SBERT(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(SBERT, self).__init__()\n",
    "        self.bert = bert         #use pre-trained BERT\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)   #average pooling for sentence embeddings\n",
    "        self.fc = nn.Linear(3 * d_model, 3)     #softmax classifier for 3 classes\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids1, segment_ids1, input_ids2, segment_ids2):\n",
    "     \n",
    "        #passing both sentences through the same BERT model as described in the paper\n",
    "        output1 = self.bert.get_last_hidden_state(input_ids1, segment_ids1)\n",
    "        output2 = self.bert.get_last_hidden_state(input_ids2, segment_ids2)\n",
    "\n",
    "        #pooling\n",
    "        u = self.pooling(output1.permute(0, 2, 1)).squeeze(-1)\n",
    "        v = self.pooling(output2.permute(0, 2, 1)).squeeze(-1)\n",
    "\n",
    "        # (u, v, |u - v|)\n",
    "        combined = torch.cat([u, v, torch.abs(u - v)], dim=1)\n",
    "\n",
    "        # Softmax classifier\n",
    "        logits = self.fc(combined)\n",
    "     \n",
    "        return logits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (bert): BERT(\n",
       "    (embedding): Embedding(\n",
       "      (tok_embed): Embedding(114008, 768)\n",
       "      (pos_embed): Embedding(1000, 768)\n",
       "      (seg_embed): Embedding(2, 768)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x EncoderLayer(\n",
       "        (enc_self_attn): MultiHeadAttention(\n",
       "          (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (pos_ffn): PoswiseFeedForwardNet(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activ): Tanh()\n",
       "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "    (decoder): Linear(in_features=768, out_features=114008, bias=False)\n",
       "  )\n",
       "  (pooling): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc): Linear(in_features=2304, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_model = SBERT(model)\n",
    "sbert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(sbert_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainning\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "      \n",
    "        input_ids1 = batch[\"input_ids1\"].to(device)\n",
    "        input_ids2 = batch[\"input_ids2\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        segment_ids1 = torch.zeros_like(input_ids1, device=device)\n",
    "        segment_ids2 = torch.zeros_like(input_ids2, device=device)\n",
    "\n",
    "        logits = sbert_model(input_ids1, segment_ids1, input_ids2, segment_ids2)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss.item():.6f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sbert_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids1 = batch[\"input_ids1\"].to(device)\n",
    "        input_ids2 = batch[\"input_ids2\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        segment_ids1 = torch.zeros_like(input_ids1).to(device)\n",
    "        segment_ids2 = torch.zeros_like(input_ids2).to(device)\n",
    "\n",
    "        logits = sbert_model(input_ids1, segment_ids1, input_ids2, segment_ids2)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyTorch Env (py 3.10)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
